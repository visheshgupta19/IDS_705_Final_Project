{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN, all_points_membership_vectors\n",
    "from keybert import KeyBERT\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn.metrics import davies_bouldin_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load & Preprocess News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_df = pd.read_csv(\"gold-dataset-sinha-khandait.csv\")\n",
    "headlines = gold_df[\"News\"].dropna().astype(str).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months = r\"\\b(january|february|march|april|may|june|july|august|september|october|november|december|jan|feb|mar|apr|jun|jul|aug|sep|oct|nov|dec)\\b\"\n",
    "directions = r\"\\b(up|down|higher|lower|rise|rises|fall|falls|gain|gains|loses|loss|rebound|slip|climb|surge|drop|drops|edged|edges|recover|recovery|recovers|flat)\\b\"\n",
    "numbers = r\"[\\d\\.,]+[%$]?|\\d{1,3}(,\\d{3})*(\\.\\d+)?|\\d+\"\n",
    "symbols = r\"\\/oz|rs|bn|usd|\\$|%|oz\"\n",
    "\n",
    "cleaned_headlines = []\n",
    "for h in headlines:\n",
    "    h = h.lower()\n",
    "    h = re.sub(months, \"\", h)\n",
    "    h = re.sub(directions, \"\", h)\n",
    "    h = re.sub(numbers, \"\", h)\n",
    "    h = re.sub(symbols, \"\", h)\n",
    "    h = re.sub(r\"[^\\w\\s]\", \"\", h)\n",
    "    h = re.sub(r\"\\s+\", \" \", h).strip()\n",
    "    cleaned_headlines.append(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Models: Embedding, Vectorizer, UMAP, HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer(\"all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    ngram_range=(1, 3),\n",
    "    min_df=10,\n",
    "    max_df=0.5,\n",
    "    max_features=5000,\n",
    "    token_pattern=r\"(?u)\\b[\\w\\-]+\\b\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_model = UMAP(\n",
    "    n_neighbors=15, n_components=5, min_dist=0.0, metric=\"cosine\", random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdbscan_model = HDBSCAN(\n",
    "    min_cluster_size=60,\n",
    "    min_samples=10,\n",
    "    cluster_selection_epsilon=0.1,\n",
    "    prediction_data=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model = BERTopic(\n",
    "    embedding_model=\"all-mpnet-base-v2\",\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    umap_model=umap_model,\n",
    "    vectorizer_model=vectorizer,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "topics, probs = topic_model.fit_transform(cleaned_headlines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft Cluster Probabilities + DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_matrix = np.array(all_points_membership_vectors(topic_model.hdbscan_model))\n",
    "normalized_prob = prob_matrix / prob_matrix.sum(axis=1, keepdims=True)\n",
    "\n",
    "prob_df = pd.DataFrame(\n",
    "    normalized_prob, columns=[str(i) for i in range(normalized_prob.shape[1])]\n",
    ")\n",
    "prob_df[\"dominant_topic\"] = prob_df.idxmax(axis=1)\n",
    "prob_df[\"topic\"] = topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map BERTopic's Internal Topic ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_mapping = topic_model.topic_mapper_.get_mappings()\n",
    "\n",
    "prob_df[\"topic\"] = (\n",
    "    pd.to_numeric(prob_df[\"topic\"], errors=\"coerce\").fillna(-1).astype(int)\n",
    ")\n",
    "prob_df[\"dominant_topic\"] = (\n",
    "    pd.to_numeric(prob_df[\"dominant_topic\"], errors=\"coerce\").fillna(-1).astype(int)\n",
    ")\n",
    "prob_df[\"mapped_topic\"] = prob_df[\"dominant_topic\"].map(topic_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agglomerative Merging of Similar Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_embeddings = topic_model.topic_embeddings_\n",
    "# similarity_matrix = cosine_similarity(topic_embeddings)\n",
    "# distance_matrix = 1 - similarity_matrix\n",
    "\n",
    "# agg_cluster = AgglomerativeClustering(\n",
    "#     n_clusters=15, metric=\"precomputed\", linkage=\"average\"\n",
    "# )\n",
    "# topic_groups = agg_cluster.fit_predict(distance_matrix)\n",
    "# print(\"Davies-Bouldin Score:\", davies_bouldin_score(distance_matrix, topic_groups))\n",
    "# topic_group_map = pd.DataFrame(\n",
    "#     {\"Original_Topic\": range(len(topic_groups)), \"New_Group\": topic_groups}\n",
    "# )\n",
    "\n",
    "# prob_df[\"merged_group\"] = prob_df[\"mapped_topic\"].map(\n",
    "#     topic_group_map.set_index(\"Original_Topic\")[\"New_Group\"]\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Metadata Back to News Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gold_df_filtered = gold_df.loc[gold_df[\"News\"].notna()].copy()\n",
    "# gold_df_filtered = pd.concat([gold_df_filtered, prob_df.reset_index(drop=True)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_topics().show()\n",
    "topic_model.visualize_barchart(top_n_topics=10).show()\n",
    "topic_model.visualize_hierarchy().show()\n",
    "topic_model.visualize_heatmap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance based cluster merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = topic_model.topic_embeddings_\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "similarity = cosine_similarity(embeddings)\n",
    "distance = 1 - similarity  # for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage, fcluster, dendrogram\n",
    "\n",
    "Z = linkage(distance, method=\"average\")  # or 'ward' if Euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "macro_labels = fcluster(Z, t=20, criterion=\"maxclust\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# macro_labels = fcluster(Z, t=0.2, criterion=\"distance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_group_map = pd.DataFrame(\n",
    "    {\"Original_Topic\": np.arange(len(macro_labels)), \"Macro_Group\": macro_labels}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_group_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_group_map[\"Macro_Group\"].value_counts().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_df[\"Macro_Group\"] = [\n",
    "    (\n",
    "        topic_group_map.loc[\n",
    "            topic_group_map[\"Original_Topic\"] == t, \"Macro_Group\"\n",
    "        ].values[0]\n",
    "        if t in topic_group_map[\"Original_Topic\"].values\n",
    "        else -1\n",
    "    )\n",
    "    for t in topics\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align topic-to-group mapping with the shape of the probability matrix\n",
    "aligned_macro_group_map = topic_group_map.loc[\n",
    "    topic_group_map[\"Original_Topic\"] < prob_matrix.shape[1]\n",
    "]\n",
    "\n",
    "# Compute macro group probabilities (via matrix multiplication)\n",
    "macro_group_prob = (\n",
    "    prob_matrix @ pd.get_dummies(aligned_macro_group_map[\"Macro_Group\"]).values\n",
    ")\n",
    "\n",
    "# Store as list in a single column\n",
    "prob_df[\"all_probabilities\"] = macro_group_prob.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Small epsilon to avoid log(0)\n",
    "epsilon = 1e-12\n",
    "\n",
    "# Convert to log-space\n",
    "log_macro_group_prob = np.log(macro_group_prob + epsilon)\n",
    "\n",
    "# Store log-space probabilities as a list in a single column\n",
    "prob_df[\"all_probabilities\"] = log_macro_group_prob.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the probability vector as a new column\n",
    "gold_df[\"Probabilities\"] = prob_df[\"all_probabilities\"]\n",
    "\n",
    "# Add the Macro_Group column\n",
    "gold_df[\"Macro_Group\"] = [\n",
    "    (\n",
    "        topic_group_map.loc[\n",
    "            topic_group_map[\"Original_Topic\"] == t, \"Macro_Group\"\n",
    "        ].values[0]\n",
    "        if t in topic_group_map[\"Original_Topic\"].values\n",
    "        else -1\n",
    "    )\n",
    "    for t in topics\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_df[\"Macro_Group\"].value_counts().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on new headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = \"Gold falls down 2 perc as silver rises\"\n",
    "cleaned = re.sub(months, \"\", test_sentence.lower())\n",
    "cleaned = re.sub(directions, \"\", cleaned)\n",
    "cleaned = re.sub(numbers, \"\", cleaned)\n",
    "cleaned = re.sub(symbols, \"\", cleaned)\n",
    "cleaned = re.sub(r\"[^\\w\\s]\", \"\", cleaned)\n",
    "cleaned = re.sub(r\"\\s+\", \" \", cleaned).strip()\n",
    "\n",
    "topic, prob = topic_model.transform([cleaned])\n",
    "merged_group = topic_group_map.loc[\n",
    "    topic_group_map[\"Original_Topic\"] == topic[0], \"Macro_Group\"\n",
    "].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
