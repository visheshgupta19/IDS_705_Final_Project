{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic.backend import BaseEmbedder\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from keybert import KeyBERT\n",
    "import hdbscan\n",
    "\n",
    "\n",
    "# Step 1: Load data\n",
    "gold_df = pd.read_csv(\"gold-dataset-sinha-khandait.csv\")\n",
    "headlines = gold_df[\"News\"].dropna().astype(str).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "headlines_lower = [h.lower() for h in headlines]\n",
    "\n",
    "months = r\"\\b(january|february|march|april|may|june|july|august|september|october|november|december|jan|feb|mar|apr|jun|jul|aug|sep|oct|nov|dec)\\b\"\n",
    "directions = r\"\\b(up|down|higher|lower|rise|rises|fall|falls|gain|gains|loses|loss|rebound|slip|climb|surge|drop|drops|edged|edges|recover|recovery|recovers|flat)\\b\"\n",
    "numbers = r\"[\\d\\.,]+[%$]?|\\d{1,3}(,\\d{3})*(\\.\\d+)?|\\d+\"\n",
    "symbols = r\"\\/oz|rs|bn|usd|\\$|%|oz\"\n",
    "\n",
    "cleaned_headlines = []\n",
    "for h in headlines_lower:\n",
    "    h_clean = re.sub(months, \"\", h, flags=re.IGNORECASE)\n",
    "    h_clean = re.sub(directions, \"\", h_clean, flags=re.IGNORECASE)\n",
    "    h_clean = re.sub(numbers, \"\", h_clean)\n",
    "    h_clean = re.sub(symbols, \"\", h_clean, flags=re.IGNORECASE)\n",
    "    h_clean = re.sub(r\"[^\\w\\s]\", \"\", h_clean)  # remove punctuation\n",
    "    h_clean = re.sub(r\"\\s+\", \" \", h_clean).strip()  # clean up spaces\n",
    "    cleaned_headlines.append(h_clean.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer(\"all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    ngram_range=(1, 3),  # uptil trigrams\n",
    "    min_df=10,  # ignore words in less than 10 headlines (0.1%)\n",
    "    max_df=0.5,  # ignore words in more than 50% headlines\n",
    "    max_features=5_000,\n",
    "    token_pattern=r\"(?u)\\b[\\w\\-]+\\b\",  # Keep hyphenated phrases (e.g., \"AI-driven\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdbscan_model = HDBSCAN(\n",
    "    min_cluster_size=60,  # Test values between 30-100\n",
    "    min_samples=10,  # Avoids micro-clusters (10-30% of min_cluster_size)\n",
    "    cluster_selection_epsilon=0.1,  # Merges nearby clusters\n",
    "    prediction_data=True,  # for soft clustering\n",
    ")\n",
    "from umap import UMAP\n",
    "\n",
    "umap_model = UMAP(\n",
    "    n_neighbors=15,\n",
    "    n_components=5,\n",
    "    min_dist=0.0,\n",
    "    metric=\"cosine\",\n",
    "    random_state=42,  # valid here!\n",
    ")\n",
    "\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=\"all-mpnet-base-v2\",\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    umap_model=umap_model,\n",
    "    vectorizer_model=vectorizer,\n",
    "    verbose=True,\n",
    "    # Added random state for reproducibility\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics, probs = topic_model.fit_transform(cleaned_headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.hdbscan_model.probabilities_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(probs == topic_model.hdbscan_model.probabilities_).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_matrix = np.array(hdbscan.all_points_membership_vectors(topic_model.hdbscan_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_prob = prob_matrix / prob_matrix.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_prob[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hdbscan_probabilities(topic_model, documents):\n",
    "    \"\"\"Get full probability distributions for HDBSCAN using soft clustering\"\"\"\n",
    "    # First ensure we have prediction data\n",
    "    if not hasattr(topic_model.hdbscan_model, \"prediction_data_\"):\n",
    "        raise ValueError(\"HDBSCAN needs to be initialized with prediction_data=True\")\n",
    "\n",
    "    # Get embeddings\n",
    "    embeddings = topic_model._extract_embeddings(documents)\n",
    "\n",
    "    # Get all cluster probabilities using HDBSCAN's soft clustering\n",
    "    soft_clusters = hdbscan.all_points_membership_vectors(topic_model.hdbscan_model)\n",
    "\n",
    "    # Convert to numpy array and normalize\n",
    "    prob_matrix = np.array(soft_clusters)\n",
    "    normalized_probs = prob_matrix / prob_matrix.sum(axis=1, keepdims=True)\n",
    "\n",
    "    return normalized_probs\n",
    "    # return prob_matrix\n",
    "\n",
    "\n",
    "# Usage:\n",
    "hdbscan_probs = get_hdbscan_probabilities(topic_model, cleaned_headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdbscan_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdbscan_probs[1] == max(hdbscan_probs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = hdbscan_probs.shape[1]\n",
    "column_names = [str(i) for i in range(0, n_topics)]\n",
    "\n",
    "prob_df = pd.DataFrame(hdbscan_probs, columns=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_columns = [col for col in prob_df.columns]\n",
    "\n",
    "prob_df[\"dominant_topic\"] = prob_df[topic_columns].idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_df[\"topic\"] = topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERTopic's topic mapping\n",
    "topic_mapping = topic_model.topic_mapper_.get_mappings()\n",
    "print(topic_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_df[\"topic\"] = (\n",
    "    pd.to_numeric(prob_df[\"topic\"], errors=\"coerce\").fillna(-1).astype(int)\n",
    ")\n",
    "prob_df[\"dominant_topic\"] = (\n",
    "    pd.to_numeric(prob_df[\"dominant_topic\"], errors=\"coerce\").fillna(-1).astype(int)\n",
    ")\n",
    "prob_df[\"mapped_topic\"] = prob_df[\"dominant_topic\"].map(topic_mapping)\n",
    "prob_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_df[prob_df[\"topic\"] == -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_df[\"matching\"] = prob_df[\"topic\"] == prob_df[\"mapped_topic\"]\n",
    "prob_df[\"matching\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "7621 / prob_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2818 + 7621 == prob_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(2818 + 7621) / 10570"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_df[prob_df[\"matching\"] == False][\"topic\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.topic_embeddings_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get topic embeddings (dimensions: n_topics x embedding_size)\n",
    "topic_embeddings = topic_model.topic_embeddings_\n",
    "topic_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Compute similarity matrix (n_topics x n_topics)\n",
    "similarity_matrix = cosine_similarity(topic_embeddings)\n",
    "similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "n_target_clusters = 10\n",
    "distance_matrix = 1 - similarity_matrix  # Convert to distance\n",
    "agg_cluster = AgglomerativeClustering(\n",
    "    n_clusters=n_target_clusters,\n",
    "    metric=\"precomputed\",  # Critical fix\n",
    "    linkage=\"average\",\n",
    ")\n",
    "topic_groups = agg_cluster.fit_predict(distance_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_group_map = pd.DataFrame(\n",
    "    {\"Original_Topic\": range(len(topic_groups)), \"New_Group\": topic_groups}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_group_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_df[\"merged_group\"] = prob_df[\"mapped_topic\"].map(\n",
    "    topic_group_map.set_index(\"Original_Topic\")[\"New_Group\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Save topics back to dataframe\n",
    "gold_df_filtered = gold_df.loc[gold_df[\"News\"].notna()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_df_filtered = pd.concat([gold_df_filtered, prob_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: View top 10 topics\n",
    "print(topic_model.get_topic_info().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_topic = 0  # change this to see different clusters\n",
    "print(f\"\\n--- Sample Headlines from Topic {sample_topic} ---\")\n",
    "print(gold_df_filtered[gold_df_filtered[\"topic\"] == sample_topic][\"News\"].head(5))\n",
    "\n",
    "# Step 6: Visualize\n",
    "topic_model.visualize_topics().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_barchart(top_n_topics=10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_hierarchy().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "Z = linkage(distance_matrix, \"average\")\n",
    "plt.figure(figsize=(12, 6))\n",
    "dendrogram(Z, labels=[f\"Topic {i}\" for i in range(len(topic_embeddings))])\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Hierarchical Clustering Dendrogram\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_heatmap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_df_filtered[\"merged_group\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_df_filtered[gold_df_filtered[\"merged_group\"] == 9][\"News\"][:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sentence = \"Gold falls down 2 perc as silver rises\"\n",
    "\n",
    "# Clean the sentence (same as training)\n",
    "cleaned_sentence = re.sub(months, \"\", new_sentence.lower())\n",
    "cleaned_sentence = re.sub(directions, \"\", cleaned_sentence)\n",
    "cleaned_sentence = re.sub(numbers, \"\", cleaned_sentence)\n",
    "cleaned_sentence = re.sub(symbols, \"\", cleaned_sentence)\n",
    "cleaned_sentence = re.sub(r\"[^\\w\\s]\", \"\", cleaned_sentence)\n",
    "cleaned_sentence = re.sub(r\"\\s+\", \" \", cleaned_sentence).strip()\n",
    "\n",
    "cleaned_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic, prob = topic_model.transform([cleaned_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dominant_topic = topic[0]\n",
    "dominant_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_group = topic_group_map[topic_group_map[\"Original_Topic\"] == dominant_topic][\n",
    "    \"New_Group\"\n",
    "].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 1: Get topic embeddings and IDs\n",
    "# topic_ids = topic_model.get_topic_info().Topic.tolist()\n",
    "# # Filter out -1 (outliers)\n",
    "# topic_ids = [t for t in topic_ids if t != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = topic_model.topic_embeddings_\n",
    "# # Only keep embeddings for the selected topic IDs\n",
    "# topic_idx_map = {i: topic_ids.index(i) for i in topic_ids}\n",
    "# filtered_embeddings = np.array([embeddings[i] for i in topic_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Compute cosine similarity matrix\n",
    "# cosine_sim = cosine_similarity(filtered_embeddings)\n",
    "\n",
    "# # Step 3: Find topic pairs with high similarity (excluding diagonal)\n",
    "# threshold = 0.85\n",
    "# highly_similar_pairs = []\n",
    "# for i, j in itertools.combinations(range(len(topic_ids)), 2):\n",
    "#     if cosine_sim[i, j] >= threshold:\n",
    "#         highly_similar_pairs.append((topic_ids[i], topic_ids[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_topics = gold_df_filtered.Topic.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 1: Build Union-Find to track connected components\n",
    "# class UnionFind:\n",
    "#     def __init__(self):\n",
    "#         self.parent = {}\n",
    "\n",
    "#     def find(self, x):\n",
    "#         if x != self.parent.setdefault(x, x):\n",
    "#             self.parent[x] = self.find(self.parent[x])\n",
    "#         return self.parent[x]\n",
    "\n",
    "#     def union(self, x, y):\n",
    "#         self.parent[self.find(y)] = self.find(x)\n",
    "\n",
    "\n",
    "# uf = UnionFind()\n",
    "# for a, b in highly_similar_pairs:\n",
    "#     uf.union(min(a, b), max(a, b))  # Always union to the smaller ID\n",
    "\n",
    "# # Step 2: Build final topic mapping to lowest ID in each group\n",
    "# # Also apply to all unique topics (including those not in pairs)\n",
    "# final_mapping = {}\n",
    "# for topic in unique_topics:\n",
    "#     if topic == -1:\n",
    "#         final_mapping[topic] = -1\n",
    "#     else:\n",
    "#         final_mapping[topic] = uf.find(topic)\n",
    "\n",
    "# # Step 3: Apply the mapping to the dataframe\n",
    "# gold_df_filtered[\"Merged_Topic\"] = gold_df_filtered[\"Topic\"].map(final_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # remove the noise data\n",
    "# noise_data = gold_df_filtered[gold_df_filtered.Merged_Topic == -1]\n",
    "\n",
    "# gold_df_filtered = gold_df_filtered[gold_df_filtered.Merged_Topic != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gold_df_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(gold_df_filtered[\"Merged_Topic\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gold_df_filtered[\"Merged_Topic\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gold_df_filtered[gold_df_filtered.Merged_Topic == 39][\"News\"]\n",
    "# seems to be downward price movement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gold_df_filtered[gold_df_filtered.Merged_Topic == 38][\"News\"]\n",
    "# macroeconomic events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gold_df_filtered[gold_df_filtered.Merged_Topic == 13][\"News\"].head(10)\n",
    "# bullish movement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gold_df_filtered[gold_df_filtered.Merged_Topic == 28][\"News\"].head(10)\n",
    "# broader market conditions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
